+++
title = "Algunos análisis sobre el estado de los LLMs al finalizar 2025"
hascode = false
date = Date(2025, 12, 27)
rss = "Algunos análisis sobre el estado de los LLMs al finalizar 2025"

tags = ["ia", "llm"]
+++

# Algunos análisis sobre el estado de los LLMs al finalizar 2025

Mucho se habló en este año sobre los LLM, derivados y similares, pero acá tengo dos buenos resúmenes:

* Resumen por Andrej Karpathy: [https://karpathy.bearblog.dev/year-in-review-2025/](https://karpathy.bearblog.dev/year-in-review-2025/)
* Un análisis de Peter Novig: [https://github.com/norvig/pytudes/blob/main/ipynb/Advent-2025-AI.ipynb](https://github.com/norvig/pytudes/blob/main/ipynb/Advent-2025-AI.ipynb)

La siguiente lectura también es interesante, aunque no totalmente imparcial, acerca de por que necesitamos construir memoria en los agentes de IA: [https://www.linkedin.com/pulse/decision-traces-agentic-operations-why-agents-need-van-schalkwyk-vhqmc/](https://www.linkedin.com/pulse/decision-traces-agentic-operations-why-agents-need-van-schalkwyk-vhqmc/).

Pero, ya que es muy posible que escalar las redes neuronales y los datos no sea el camino a la AGI, sino que se necesite enfoques que combinen los desarrollos actuales con IA simbólica, siempre es bueno escuchar a [Gary Marcus](https://garymarcus.substack.com/): [https://garymarcus.substack.com/p/a-case-for-ai-models-that-understand](https://garymarcus.substack.com/p/a-case-for-ai-models-that-understand). En la misma línea, un análisis crítico sobre la el uso de LLM en análisis de datos: [https://journals.sagepub.com/doi/epub/10.1177/10944281251377154](https://journals.sagepub.com/doi/epub/10.1177/10944281251377154)